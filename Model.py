# chess_model_reworked.py
# –ü–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ —Å –∫–æ–Ω–≤–æ–ª—é—Ü–∏—è–º–∏ ‚Üí dense ‚Üí self‚Äëattention (Markov input).

import torch
import torch.nn as nn
import torch.nn.functional as F

device = torch.device(
    'mps' if torch.backends.mps.is_available()
    else 'cuda' if torch.cuda.is_available()
    else 'cpu'
)

# ------------------------------------------------------------------
# üî∏ Inception‚Äë–ø–æ–¥–æ–±–Ω—ã–π –º–Ω–æ–≥–æ‚Äë–º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–ª–æ–∫ (3√ó3,‚ÄØ5√ó5,‚ÄØ7√ó7) –±–µ–∑ BN
# ------------------------------------------------------------------
class MultiScaleConvBlock(nn.Module):
    def __init__(self, in_channels: int,
                 c3: int = 64, c5: int = 64, c7: int = 64):
        super().__init__()
        self.branch3 = nn.Sequential(
            nn.Conv2d(in_channels, c3, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(c3, c3, 3, padding=1),
            nn.ReLU(inplace=True),
        )
        self.branch5 = nn.Sequential(
            nn.Conv2d(in_channels, c5, 5, padding=2),
            nn.ReLU(inplace=True),
            nn.Conv2d(c5, c5, 5, padding=2),
            nn.ReLU(inplace=True),
        )
        self.branch7 = nn.Sequential(
            nn.Conv2d(in_channels, c7, 7, padding=3),
            nn.ReLU(inplace=True),
            nn.Conv2d(c7, c7, 7, padding=3),
            nn.ReLU(inplace=True),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return torch.cat([
            self.branch3(x),
            self.branch5(x),
            self.branch7(x)
        ], dim=1)  # (B, 192, 8, 8)


# ------------------------------------------------------------------
# üî∏ –ü—Ä–æ—Å—Ç–æ–π residual –±–ª–æ–∫ 3√ó3 ‚Üí 3√ó3 –±–µ–∑ BN
# ------------------------------------------------------------------
class ResidualBlock(nn.Module):
    def __init__(self, channels: int, dropout: float = 0.0):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.dropout = nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        out = F.relu(x)
        out = self.conv1(out)
        out = F.relu(out)
        out = self.dropout(self.conv2(out))
        return x + out  # skip‚Äëconnection


# ------------------------------------------------------------------
# üî∏ ChessModel
# ------------------------------------------------------------------
class ChessModel(nn.Module):
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç:
        board_x  ‚Äì (B,65,16)     —Ç–µ–Ω–∑–æ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è
        mask     ‚Äì (B,4672) bool –º–∞—Å–∫–∞ –ª–µ–≥–∞–ª—å–Ω—ã—Ö —Ö–æ–¥–æ–≤
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        logits     ‚Äì (B,4672)
    """
    def __init__(self,
                 in_channels: int = 256,
                 token_dim: int = 512,
                 n_heads: int = 16,
                 n_attn_layers: int = 6,
                 n_res_blocks: int = 4,
                 dropout: float = 0.):
        super().__init__()
        self.token_dim = token_dim
        self.dense_dim = token_dim // 4
        self.conv_dim  = 3 * token_dim // 4

        # 1) conv‚Äë–≤–µ—Ç–∫–∞ + MultiScale + residual tower
        self.stem = nn.Sequential(
            nn.Conv2d(16, in_channels, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        self.ms = MultiScaleConvBlock(
            in_channels, self.dense_dim, self.dense_dim, self.dense_dim
        )
        self.squeeze   = nn.Conv2d(self.conv_dim, self.conv_dim, 1)
        self.res_tower = nn.Sequential(
            *[ResidualBlock(self.conv_dim, dropout) for _ in range(n_res_blocks)]
        )

        # 2) dense‚Äë–≤–µ—Ç–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–ª–µ—Ç–∫–∏
        self.cell_dense = nn.Sequential(
            nn.Linear(16, self.dense_dim), nn.ReLU(inplace=True),
            nn.Linear(self.dense_dim, self.dense_dim), nn.ReLU(inplace=True)
        )

        # 3) —Å–ø–µ—Ü‚Äë—Ç–æ–∫–µ–Ω –∏–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ–ª–µ
        self.special_embed = nn.Linear(16, token_dim)

        # 4) Transformer encoder
        enc_layer = nn.TransformerEncoderLayer(
            d_model=token_dim,
            nhead=n_heads,
            dim_feedforward=4*token_dim,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(enc_layer, n_attn_layers)

        # 5) –ø–æ–ª–∏—Ç–∏–∫–∞ –∏ value‚ÄëMLP
        self.repr_mlp = nn.Sequential(
            nn.Linear(token_dim, 2*token_dim), nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(2*token_dim, token_dim), nn.ReLU(inplace=True)
        )
        self.policy_head = nn.Sequential(
            nn.Linear(token_dim, 1024), nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(1024, 4672)
        )

        self.value_head = nn.Sequential(
            nn.Linear(token_dim, 1024), nn.ReLU(),
            nn.Linear(1024, 1024), nn.ReLU(),
            nn.Linear(1024, 1)
        )


    def representation(self, x: torch.Tensor) -> torch.Tensor:
        """
        x ‚Äì (B,65,16)
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç h ‚Äì (B,token_dim) –¥–ª—è –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ value
        """
        B = x.size(0)
        special, cells = x[:, 0, :], x[:, 1:, :]  # (B,16) –∏ (B,64,16)

        # 1) —Å–≤—ë—Ä—Ç–∫–∏ ‚Üí –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feat = cells.view(B, 8, 8, 16).permute(0, 3, 1, 2)  # (B,16,8,8)
        y = self.squeeze(self.ms(self.stem(feat)))         # (B,conv_dim,8,8)
        y = self.res_tower(y)                              # (B,conv_dim,8,8)
        y = y.view(B, self.conv_dim, 64).transpose(1, 2)   # (B,64,conv_dim)

        # 2) dense‚Äë–ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–ª–µ—Ç–æ–∫
        d = self.cell_dense(cells)                         # (B,64,dense_dim)

        # 3) –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤ —Ç–æ–∫–µ–Ω—ã –∫–ª–µ—Ç–æ–∫
        tokens_cells = torch.cat([y, d], dim=-1)           # (B,64,token_dim)

        # 4) –≥–æ—Ç–æ–≤–∏–º –≤—Ö–æ–¥ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
        cls_tok = self.special_embed(special).unsqueeze(1)  # (B,1,token_dim)
        seq = torch.cat([cls_tok, tokens_cells], dim=1)
        #          ‚îî‚îÄ‚îÄ  index=0      index=1..64   ‚îÄ‚îÄ‚îò

        # 5) –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ Transformer
        seq = self.transformer(seq)  # (B,65,token_dim)

        # 6) –≤—ã–Ω–∏–º–∞–µ–º –≤—ã—Ö–æ–¥—ã
        h = self.repr_mlp(seq[:, 0, :])  # –∏–∑ CLS
        return h

    def forward(self,
                board_x: torch.Tensor,
                mask: torch.Tensor | None = None
                ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        board_x  ‚Äì (B,65,16)
        mask     ‚Äì (B,4672)
        """
        h = self.representation(board_x)
        logits = self.policy_head(h)
        value = self.value_head(h).squeeze(-1)

        if mask is not None:
            logits = logits.masked_fill(~mask, float('-inf'))
        return logits, value


# ------------------------------------------------------------------
# üî∏ –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
# ------------------------------------------------------------------
if __name__ == "__main__":
    model = ChessModel().to(device)
    dummy_x     = torch.zeros((2, 65, 16), device=device)
    logits, value = model(dummy_x)
    print("logits shape:", logits.shape)    # (2,4672)
    print("value shape:", value.shape)      # (2,)
    print("‚âà # params:", sum(p.numel() for p in model.parameters()))
